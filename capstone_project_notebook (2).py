# -*- coding: utf-8 -*-
"""CAPSTONE Project Notebook

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/capstone-project-notebook-b93cefcf-ff7e-4287-9c95-148b3b7ff55d.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240825/auto/storage/goog4_request%26X-Goog-Date%3D20240825T062907Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D9cec11142495ac29d78d303165991a1e0e60d1280d7eade7188b0325fde62928cc7d34521861e6b96f4db20c49a7fe81fbf1efca355354ed26713c4d6130675e5763d6fce30bf8cbdfc21ef01ebd5d05418bf916ab85850e1946259823cbf27bb70fa8e48e71aa4ab711eb72c63d14e5ed7feeb1d1e62183bc4e6b5468a673a47956402af14a171f87568284bcdb4e842bb43a3a16b61a17a6fa447f00aa1d43eaa263c211512ea902317e6f98441aa2c09fd7db7bb0ed943bf261ca488a0b62719593be4e174543c91ab9657b4e03402fe9b03425a43ee073bf4da3e9dec0debf3f9c5aaafe07d30f13e9a7b8b5e6c0ce35ad49c3c3eda2a2a387d9361cd941
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'leap-atmospheric-physics-ai-climsim:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F56537%2F8877088%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240825%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240825T062907Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D95318c9d566bb999dc0913247995d3206a13fe3f77b7fc7a7b9a6387dd52b959a69c4297aa2b24cc26f6f77deee5a72e70a771dbe7c42c46b62da6d20f235d21b185a740063ca674c71ab3730f3f0c5502024ddcf740c7174e854ce4776c44c4a98de171a2def5bc5eaaf98b5b65593952758642b06c30c851d16e43d07d71ae31756e0da4d5ec3bb1d76a82293d78ba823ab0442b7a6b92ef7d060cc6fc45d8a01440afad4316c19962e9f26b5f758bc87dc3ebf0c35ac83b8cd5dde94f18ff26ee8bd6de32cfdc7a0ff8acfe62b52a8455bcacd193ff258f014e8126ddbee4e8545148867512b16f4cd2e7ea723ca57fc21d14d4e41ee0d0d2e091c12b3166'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

"""# CAPSTONE Project: Atmospheric Physics using AI (ClimSim)
## Samanyu Parvathaneni

## Abstract
Climate models are crucial for understanding Earth's climate system. Due to the complexity of Earth's climate, these models use parameterizations to approximate the effects of physical processes that occur at scales smaller than the size of their grid cells. These approximations, however, are imperfect and contribute significantly to uncertainties in predicted warming, changing precipitation patterns, and the frequency and severity of extreme events. The Multi-scale Modeling Framework (MMF) approach, however, represents these subgrid processes more explicitly but at a computational cost too high for operational climate prediction.

This project aims to develop machine learning (ML) models that emulate subgrid atmospheric processes—such as storms, clouds, turbulence, rainfall, and radiation—within E3SM-MMF, a multi-scale climate model supported by the U.S. Department of Energy. ML emulators are significantly cheaper to run than MMF, and advancements in this area could enable high-resolution, physically credible long-term climate projections to become broadly accessible. This would enhance our understanding of climate-related hazards and empower policymakers with the knowledge needed to mitigate them.

## Research Question

Motivation: With the increasing frequency and severity of climate disasters, governments and other institutions are investing more time and money into projects that will be able to predict these catastrophies. We aim to model these weather events and figue out what features are the most predictive of natural disasters.

Research Question: What relationships can we find between observable weather data and climate events?

## Hypothesis

We believe that there will be many highly correlated variables, particularly the ones with the same number of dimensions.

## Data Preparation
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import dask as dd
import polars as p1
import ray
import gc  # Garbage Collector interface
import random
import time
# import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras import layers, regularizers, models
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import r2_score

# Plots
import matplotlib.pyplot as plt
import seaborn as sns

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

"""## Exploratory Data Analysis (EDA)"""

train_df = p1.read_csv('../input/leap-atmospheric-physics-ai-climsim/train.csv', n_rows = 100000).to_pandas()
test_df = p1.read_csv('../input/leap-atmospheric-physics-ai-climsim/test.csv', n_rows = 100000).to_pandas()

train_df.head(10)

test_df.head(10)

# targets (extract from submission file)
targets = [x for x in submission_df.columns if x not in ['sample_id']]

# numerical features
numerical_features = [x for x in train_df.columns if x not in ['sample_id']+targets]

# categorical features
categorical_features = []

# all features combined
features_all = numerical_features + categorical_features

# Output of dimensions
print('# of numerical features: ', len(numerical_features))
print('# of categorical features: ', len(categorical_features))
print('# of targets: ', len(targets))

train_df[targets].describe()

# Plot target distributions in compact matrix form
fig, axs = plt.subplots(92, 4, figsize=(16, 350))
i = 0
for t in targets:
    current_ax = axs.flat[i]
    current_ax.hist(train_df[t], bins=100, color='darkgreen')
    current_ax.set_title('Target' + str(t))
    current_ax.grid()
    i += 1

# Basic stats - Train
train_df[numerical_features].describe()

# Basic stats - Test
test_df[numerical_features].describe()

# Plot histograms for numerical features (train and test)
for f in numerical_features:
    plt.figure(figsize=(12,2))
    ax1 = plt.subplot(1,2,1)
    train_df[f].plot(kind='hist', bins=100, color='darkblue')
    plt.title(f + ' - Train')
    plt.grid()
    ax2 = plt.subplot(1,2,2, sharex=ax1)
    test_df[f].plot(kind='hist', bins=100, color='darkred')
    plt.title(f + ' - Test')
    plt.grid()
    # plt.show()

# compact boxplot of all features - train only
n_plot_rows = 10
n_plot_cols = 60
n = len(numerical_features)
for i in range(n_plot_rows):
    a = n_plot_cols*i+1
    b = min(n_plot_cols*i+n_plot_cols, n)
    print('Columns', a, 'to', b)
    train_df.iloc[:,a:(b+1)].plot(kind='box', figsize=(15,5))
    plt.xticks(rotation=90)
    plt.grid()
    plt.show()

for f in numerical_features:
    plt.figure(figsize=(14,0.5))
    ax1 = plt.subplot(1,2,1)
    temp_df = train_df[f].dropna() # boxplot does not like missings...
    plt.boxplot(temp_df, vert=False)
    plt.title(f + ' - Train')
    plt.grid()
    ax2 = plt.subplot(1,2,2, sharex=ax1)
    temp_df = test_df[f].dropna()
    plt.boxplot(temp_df, vert=False)
    plt.title(f + ' - Test')
    plt.grid()
    plt.show()

# Calc and plot correlation matrix
cor_p_target = train_df[targets].corr(method='pearson')
plt.figure(figsize=(14,12))
sns.heatmap(cor_p_target, annot=False, cmap='RdYlGn',
            vmin=-1, vmax=+1)
plt.title('Targets - Pearson Correlation')
plt.show()

# Calc and plot correlation matrix
cor_p_train = train_df[numerical_features].corr(method='pearson')
plt.figure(figsize=(14,12))
sns.heatmap(cor_p_train, annot=False, cmap='RdYlGn',
            vmin=-1, vmax=+1)
plt.title('Features - Pearson Correlation (train)')
plt.show()

# Calc and plot correlation matrix
cor_p_test = test_df[numerical_features].corr(method='pearson')
plt.figure(figsize=(14,12))
sns.heatmap(cor_p_test, annot=False, cmap='RdYlGn',
            vmin=-1, vmax=+1)
plt.title('Features - Pearson Correlation (test)')
plt.show()

# Plot target values
for t in targets:
    plt.figure(figsize=(14,2))
    plt.scatter(train_df.index, train_df[t], color='darkblue',
                alpha=0.25, s=1)
    plt.title(t)
    plt.grid()
    plt.show()

# Define columns (has to be a list)
n_max = 20 # columns with index 0..n_max
cols_select = ['state_t_' + str(t) for t in range(0,n_max+1)]
print(cols_select)

# Load only selected column
t1 = time.time()
df_col = p1.read_csv('../input/leap-atmospheric-physics-ai-climsim/train.csv', columns=cols_select).to_pandas()
t2 = time.time()
print('Elapsed time [s]: ', np.round(t2-t1,2))
print('Number of rows: ', df_col.shape[0])
print('Number of cols: ', df_col.shape[1])

# Basic stats
df_col.describe(percentiles=[0.01,0.1,0.25,0.5,0.75,0.9,0.99])

# plot distributions
for f in cols_select:
    plt.figure(figsize=(10,3))
    plt.hist(df_col[f], bins=1000, color='darkred')
    plt.title(f + ' - full data')
    plt.grid()
    plt.show()

# Boxplot
plt.figure(figsize=(10,0.5))
plt.boxplot(df_col.state_t_0, vert=False)
plt.title('state_t_0')
plt.grid()
plt.show()

# Let's check the most extreme outliers
df_col[df_col.state_t_0>400]

# Calc and plot correlation matrix
cor_p_train_few_cols = train_df[cols_select].corr(method='pearson')
plt.figure(figsize=(14,10))
sns.heatmap(cor_p_train_few_cols, annot=True, cmap='RdYlGn',
            fmt='.2f', linecolor='black', linewidth=.5,
            vmin=-1, vmax=+1)
plt.title('Features - Pearson Correlation (train/selected columns)')
plt.show()

"""## Model Training

#### Neural Network (Keras) with ReLU activation, batch normalization, and dropout
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras import layers, regularizers, models
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import r2_score
import matplotlib.pyplot as plt

# Assume train_df and test_df are already loaded with the specified columns

# Separate features and targets
feature_columns = ['state_t_0', 'state_t_1', 'state_t_2', 'state_t_3', 'state_t_4', 'state_t_5', 'state_t_6', 'state_t_7', 'state_t_8','state_t_9',
                   'state_t_10', 'state_t_11', 'state_t_12', 'state_t_13', 'state_t_14', 'state_t_15', 'state_t_16', 'state_t_17', 'state_t_18', 'state_t_19',
                   'state_t_20','state_t_21', 'state_t_22', 'state_t_23', 'state_t_24', 'state_t_25', 'state_t_26', 'state_t_27', 'state_t_28', 'state_t_29',
                   'state_t_30', 'state_t_31', 'state_t_32', 'state_t_33', 'state_t_34', 'state_t_35', 'state_t_36', 'state_t_37', 'state_t_38', 'state_t_39',
                   'state_t_40', 'state_t_41', 'state_t_42', 'state_t_43', 'state_t_44', 'state_t_45', 'state_t_46', 'state_t_47', 'state_t_48', 'state_t_49',
                   'state_t_50', 'state_t_51', 'state_t_52', 'state_t_53', 'state_t_54', 'state_t_55', 'state_t_56', 'state_t_57', 'state_t_58','state_v_59',
                   'state_q0001_0', 'state_q0001_1', 'state_q0001_2', 'state_q0001_3', 'state_q0001_4', 'state_q0001_5', 'state_q0001_6', 'state_q0001_7', 'state_q0001_8','state_q0001_9',
                   'state_q0001_10', 'state_q0001_11', 'state_q0001_12', 'state_q0001_13', 'state_q0001_14', 'state_q0001_15', 'state_q0001_16', 'state_q0001_17', 'state_q0001_18', 'state_q0001_19',
                   'state_q0001_20','state_q0001_21', 'state_q0001_22', 'state_q0001_23', 'state_q0001_24', 'state_q0001_25', 'state_q0001_26', 'state_q0001_27', 'state_q0001_28', 'state_q0001_29',
                   'state_q0001_30', 'state_q0001_31', 'state_q0001_32', 'state_q0001_33', 'state_q0001_34', 'state_q0001_35', 'state_q0001_36', 'state_q0001_37', 'state_q0001_38', 'state_q0001_39',
                   'state_q0001_40', 'state_q0001_41', 'state_q0001_42', 'state_q0001_43', 'state_q0001_44', 'state_q0001_45', 'state_q0001_46', 'state_q0001_47', 'state_q0001_48', 'state_q0001_49',
                   'state_q0001_50', 'state_q0001_51', 'state_q0001_52', 'state_q0001_53', 'state_q0001_54', 'state_q0001_55', 'state_q0001_56', 'state_q0001_57', 'state_q0001_58','state_v_59',
                   'state_q0002_0', 'state_q0002_1', 'state_q0002_2', 'state_q0002_3', 'state_q0002_4', 'state_q0002_5', 'state_q0002_6', 'state_q0002_7', 'state_q0002_8','state_q0002_9',
                   'state_q0002_10', 'state_q0002_11', 'state_q0002_12', 'state_q0002_13', 'state_q0002_14', 'state_q0002_15', 'state_q0002_16', 'state_q0002_17', 'state_q0002_18', 'state_q0002_19',
                   'state_q0002_20','state_q0002_21', 'state_q0002_22', 'state_q0002_23', 'state_q0002_24', 'state_q0002_25', 'state_q0002_26', 'state_q0002_27', 'state_q0002_28', 'state_q0002_29',
                   'state_q0002_30', 'state_q0002_31', 'state_q0002_32', 'state_q0002_33', 'state_q0002_34', 'state_q0002_35', 'state_q0002_36', 'state_q0002_37', 'state_q0002_38', 'state_q0002_39',
                   'state_q0002_40', 'state_q0002_41', 'state_q0002_42', 'state_q0002_43', 'state_q0002_44', 'state_q0002_45', 'state_q0002_46', 'state_q0002_47', 'state_q0002_48', 'state_q0002_49',
                   'state_q0002_50', 'state_q0002_51', 'state_q0002_52', 'state_q0002_53', 'state_q0002_54', 'state_q0002_55', 'state_q0002_56', 'state_q0002_57', 'state_q0002_58','state_v_59',
                   'state_q0003_0', 'state_q0003_1', 'state_q0003_2', 'state_q0003_3', 'state_q0003_4', 'state_q0003_5', 'state_q0003_6', 'state_q0003_7', 'state_q0003_8','state_q0003_9',
                   'state_q0003_10', 'state_q0003_11', 'state_q0003_12', 'state_q0003_13', 'state_q0003_14', 'state_q0003_15', 'state_q0003_16', 'state_q0003_17', 'state_q0003_18', 'state_q0003_19',
                   'state_q0003_20','state_q0003_21', 'state_q0003_22', 'state_q0003_23', 'state_q0003_24', 'state_q0003_25', 'state_q0003_26', 'state_q0003_27', 'state_q0003_28', 'state_q0003_29',
                   'state_q0003_30', 'state_q0003_31', 'state_q0003_32', 'state_q0003_33', 'state_q0003_34', 'state_q0003_35', 'state_q0003_36', 'state_q0003_37', 'state_q0003_38', 'state_q0003_39',
                   'state_q0003_40', 'state_q0003_41', 'state_q0003_42', 'state_q0003_43', 'state_q0003_44', 'state_q0003_45', 'state_q0003_46', 'state_q0003_47', 'state_q0003_48', 'state_q0003_49',
                   'state_q0003_50', 'state_q0003_51', 'state_q0003_52', 'state_q0003_53', 'state_q0003_54', 'state_q0003_55', 'state_q0003_56', 'state_q0003_57', 'state_q0003_58','state_v_59',
                   'state_u_0', 'state_u_1', 'state_u_2', 'state_u_3', 'state_u_4', 'state_u_5', 'state_u_6', 'state_u_7', 'state_u_8','state_u_9',
                   'state_u_10', 'state_u_11', 'state_u_12', 'state_u_13', 'state_u_14', 'state_u_15', 'state_u_16', 'state_u_17', 'state_u_18', 'state_u_19',
                   'state_u_20','state_u_21', 'state_u_22', 'state_u_23', 'state_u_24', 'state_u_25', 'state_u_26', 'state_u_27', 'state_u_28', 'state_u_29',
                   'state_u_30', 'state_u_31', 'state_u_32', 'state_u_33', 'state_u_34', 'state_u_35', 'state_u_36', 'state_u_37', 'state_u_38', 'state_u_39',
                   'state_u_40', 'state_u_41', 'state_u_42', 'state_u_43', 'state_u_44', 'state_u_45', 'state_u_46', 'state_u_47', 'state_u_48', 'state_u_49',
                   'state_u_50', 'state_u_51', 'state_u_52', 'state_u_53', 'state_u_54', 'state_u_55', 'state_u_56', 'state_u_57', 'state_u_58','state_v_59',
                   'state_v_0', 'state_v_1', 'state_v_2', 'state_v_3', 'state_v_4', 'state_v_5', 'state_v_6', 'state_v_7', 'state_v_8','state_v_9',
                   'state_v_10', 'state_v_11', 'state_v_12', 'state_v_13', 'state_v_14', 'state_v_15', 'state_v_16', 'state_v_17', 'state_v_18', 'state_v_19',
                   'state_v_20','state_v_21', 'state_v_22', 'state_v_23', 'state_v_24', 'state_v_25', 'state_v_26', 'state_v_27', 'state_v_28', 'state_v_29',
                   'state_v_30', 'state_v_31', 'state_v_32', 'state_v_33', 'state_v_34', 'state_v_35', 'state_v_36', 'state_v_37', 'state_v_38', 'state_v_39',
                   'state_v_40', 'state_v_41', 'state_v_42', 'state_v_43', 'state_v_44', 'state_v_45', 'state_v_46', 'state_v_47', 'state_v_48', 'state_v_49',
                   'state_v_50', 'state_v_51', 'state_v_52', 'state_v_53', 'state_v_54', 'state_v_55', 'state_v_56', 'state_v_57', 'state_v_58','state_v_59',
                   'state_ps', 'pbuf_SOLIN', 'pbuf_LHFLX', 'pbuf_TAUX', 'pbuf_TAUY', 'pbuf_COSZRS', 'cam_in_ALDIF', 'cam_in_ASDIF', 'cam_in_LWUP', 'cam_in_ICEFRAC', 'cam_in_LANDFRAC', 'cam_in_OCNFRAC', 'cam_in_SNOWHLAND',
                   'pbuf_ozone_0', 'pbuf_ozone_1', 'pbuf_ozone_2', 'pbuf_ozone_3', 'pbuf_ozone_4', 'pbuf_ozone_5', 'pbuf_ozone_6', 'pbuf_ozone_7', 'pbuf_ozone_8','pbuf_ozone_9',
                   'pbuf_ozone_10', 'pbuf_ozone_11', 'pbuf_ozone_12', 'pbuf_ozone_13', 'pbuf_ozone_14', 'pbuf_ozone_15', 'pbuf_ozone_16', 'pbuf_ozone_17', 'pbuf_ozone_18', 'pbuf_ozone_19',
                   'pbuf_ozone_20','pbuf_ozone_21', 'pbuf_ozone_22', 'pbuf_ozone_23', 'pbuf_ozone_24', 'pbuf_ozone_25', 'pbuf_ozone_26', 'pbuf_ozone_27', 'pbuf_ozone_28', 'pbuf_ozone_29',
                   'pbuf_ozone_30', 'pbuf_ozone_31', 'pbuf_ozone_32', 'pbuf_ozone_33', 'pbuf_ozone_34', 'pbuf_ozone_35', 'pbuf_ozone_36', 'pbuf_ozone_37', 'pbuf_ozone_38', 'pbuf_ozone_39',
                   'pbuf_ozone_40', 'pbuf_ozone_41', 'pbuf_ozone_42', 'pbuf_ozone_43', 'pbuf_ozone_44', 'pbuf_ozone_45', 'pbuf_ozone_46', 'pbuf_ozone_47', 'pbuf_ozone_48', 'pbuf_ozone_49',
                   'pbuf_ozone_50', 'pbuf_ozone_51', 'pbuf_ozone_52', 'pbuf_ozone_53', 'pbuf_ozone_54', 'pbuf_ozone_55', 'pbuf_ozone_56', 'pbuf_ozone_57', 'pbuf_ozone_58','pbuf_ozone_59',
                   'pbuf_CH4_0', 'pbuf_CH4_1', 'pbuf_CH4_2', 'pbuf_CH4_3', 'pbuf_CH4_4', 'pbuf_CH4_5', 'pbuf_CH4_6', 'pbuf_CH4_7', 'pbuf_CH4_8','pbuf_CH4_9',
                   'pbuf_CH4_10', 'pbuf_CH4_11', 'pbuf_CH4_12', 'pbuf_CH4_13', 'pbuf_CH4_14', 'pbuf_CH4_15', 'pbuf_CH4_16', 'pbuf_CH4_17', 'pbuf_CH4_18', 'pbuf_CH4_19',
                   'pbuf_CH4_20','pbuf_CH4_21', 'pbuf_CH4_22', 'pbuf_CH4_23', 'pbuf_CH4_24', 'pbuf_CH4_25', 'pbuf_CH4_26', 'pbuf_CH4_27', 'pbuf_CH4_28', 'pbuf_CH4_29',
                   'pbuf_CH4_30', 'pbuf_CH4_31', 'pbuf_CH4_32', 'pbuf_CH4_33', 'pbuf_CH4_34', 'pbuf_CH4_35', 'pbuf_CH4_36', 'pbuf_CH4_37', 'pbuf_CH4_38', 'pbuf_CH4_39',
                   'pbuf_CH4_40', 'pbuf_CH4_41', 'pbuf_CH4_42', 'pbuf_CH4_43', 'pbuf_CH4_44', 'pbuf_CH4_45', 'pbuf_CH4_46', 'pbuf_CH4_47', 'pbuf_CH4_48', 'pbuf_CH4_49',
                   'pbuf_CH4_50', 'pbuf_CH4_51', 'pbuf_CH4_52', 'pbuf_CH4_53', 'pbuf_CH4_54', 'pbuf_CH4_55', 'pbuf_CH4_56', 'pbuf_CH4_57', 'pbuf_CH4_58','pbuf_CH4_59',
                   'pbuf_N2O_0', 'pbuf_N2O_1', 'pbuf_N2O_2', 'pbuf_N2O_3', 'pbuf_N2O_4', 'pbuf_N2O_5', 'pbuf_N2O_6', 'pbuf_N2O_7', 'pbuf_N2O_8','pbuf_N2O_9',
                   'pbuf_N2O_10', 'pbuf_N2O_11', 'pbuf_N2O_12', 'pbuf_N2O_13', 'pbuf_N2O_14', 'pbuf_N2O_15', 'pbuf_N2O_16', 'pbuf_N2O_17', 'pbuf_N2O_18', 'pbuf_N2O_19',
                   'pbuf_N2O_20','pbuf_N2O_21', 'pbuf_N2O_22', 'pbuf_N2O_23', 'pbuf_N2O_24', 'pbuf_N2O_25', 'pbuf_N2O_26', 'pbuf_N2O_27', 'pbuf_N2O_28', 'pbuf_N2O_29',
                   'pbuf_N2O_30', 'pbuf_N2O_31', 'pbuf_N2O_32', 'pbuf_N2O_33', 'pbuf_N2O_34', 'pbuf_N2O_35', 'pbuf_N2O_36', 'pbuf_N2O_37', 'pbuf_N2O_38', 'pbuf_N2O_39',
                   'pbuf_N2O_40', 'pbuf_N2O_41', 'pbuf_N2O_42', 'pbuf_N2O_43', 'pbuf_N2O_44', 'pbuf_N2O_45', 'pbuf_N2O_46', 'pbuf_N2O_47', 'pbuf_N2O_48', 'pbuf_N2O_49',
                   'pbuf_N2O_50', 'pbuf_N2O_51', 'pbuf_N2O_52', 'pbuf_N2O_53', 'pbuf_N2O_54', 'pbuf_N2O_55', 'pbuf_N2O_56', 'pbuf_N2O_57', 'pbuf_N2O_58','pbuf_N2O_59']

target_columns = ['ptend_t_0', 'ptend_t_1', 'ptend_t_2', 'ptend_t_3', 'ptend_t_4', 'ptend_t_5', 'ptend_t_6', 'ptend_t_7', 'ptend_t_8','ptend_t_9',
                   'ptend_t_10', 'ptend_t_11', 'ptend_t_12', 'ptend_t_13', 'ptend_t_14', 'ptend_t_15', 'ptend_t_16', 'ptend_t_17', 'ptend_t_18', 'ptend_t_19',
                   'ptend_t_20','ptend_t_21', 'ptend_t_22', 'ptend_t_23', 'ptend_t_24', 'ptend_t_25', 'ptend_t_26', 'ptend_t_27', 'ptend_t_28', 'ptend_t_29',
                   'ptend_t_30', 'ptend_t_31', 'ptend_t_32', 'ptend_t_33', 'ptend_t_34', 'ptend_t_35', 'ptend_t_36', 'ptend_t_37', 'ptend_t_38', 'ptend_t_39',
                   'ptend_t_40', 'ptend_t_41', 'ptend_t_42', 'ptend_t_43', 'ptend_t_44', 'ptend_t_45', 'ptend_t_46', 'ptend_t_47', 'ptend_t_48', 'ptend_t_49',
                   'ptend_t_50', 'ptend_t_51', 'ptend_t_52', 'ptend_t_53', 'ptend_t_54', 'ptend_t_55', 'ptend_t_56', 'ptend_t_57', 'ptend_t_58','ptend_t_59',
                  'ptend_q0001_0', 'ptend_q0001_1', 'ptend_q0001_2', 'ptend_q0001_3', 'ptend_q0001_4', 'ptend_q0001_5', 'ptend_q0001_6', 'ptend_q0001_7', 'ptend_q0001_8','ptend_q0001_9',
                   'ptend_q0001_10', 'ptend_q0001_11', 'ptend_q0001_12', 'ptend_q0001_13', 'ptend_q0001_14', 'ptend_q0001_15', 'ptend_q0001_16', 'ptend_q0001_17', 'ptend_q0001_18', 'ptend_q0001_19',
                   'ptend_q0001_20','ptend_q0001_21', 'ptend_q0001_22', 'ptend_q0001_23', 'ptend_q0001_24', 'ptend_q0001_25', 'ptend_q0001_26', 'ptend_q0001_27', 'ptend_q0001_28', 'ptend_q0001_29',
                   'ptend_q0001_30', 'ptend_q0001_31', 'ptend_q0001_32', 'ptend_q0001_33', 'ptend_q0001_34', 'ptend_q0001_35', 'ptend_q0001_36', 'ptend_q0001_37', 'ptend_q0001_38', 'ptend_q0001_39',
                   'ptend_q0001_40', 'ptend_q0001_41', 'ptend_q0001_42', 'ptend_q0001_43', 'ptend_q0001_44', 'ptend_q0001_45', 'ptend_q0001_46', 'ptend_q0001_47', 'ptend_q0001_48', 'ptend_q0001_49',
                   'ptend_q0001_50', 'ptend_q0001_51', 'ptend_q0001_52', 'ptend_q0001_53', 'ptend_q0001_54', 'ptend_q0001_55', 'ptend_q0001_56', 'ptend_q0001_57', 'ptend_q0001_58','ptend_q0001_59',
                  'ptend_q0002_0', 'ptend_q0002_1', 'ptend_q0002_2', 'ptend_q0002_3', 'ptend_q0002_4', 'ptend_q0002_5', 'ptend_q0002_6', 'ptend_q0002_7', 'ptend_q0002_8','ptend_q0002_9',
                   'ptend_q0002_10', 'ptend_q0002_11', 'ptend_q0002_12', 'ptend_q0002_13', 'ptend_q0002_14', 'ptend_q0002_15', 'ptend_q0002_16', 'ptend_q0002_17', 'ptend_q0002_18', 'ptend_q0002_19',
                   'ptend_q0002_20','ptend_q0002_21', 'ptend_q0002_22', 'ptend_q0002_23', 'ptend_q0002_24', 'ptend_q0002_25', 'ptend_q0002_26', 'ptend_q0002_27', 'ptend_q0002_28', 'ptend_q0002_29',
                   'ptend_q0002_30', 'ptend_q0002_31', 'ptend_q0002_32', 'ptend_q0002_33', 'ptend_q0002_34', 'ptend_q0002_35', 'ptend_q0002_36', 'ptend_q0002_37', 'ptend_q0002_38', 'ptend_q0002_39',
                   'ptend_q0002_40', 'ptend_q0002_41', 'ptend_q0002_42', 'ptend_q0002_43', 'ptend_q0002_44', 'ptend_q0002_45', 'ptend_q0002_46', 'ptend_q0002_47', 'ptend_q0002_48', 'ptend_q0002_49',
                   'ptend_q0002_50', 'ptend_q0002_51', 'ptend_q0002_52', 'ptend_q0002_53', 'ptend_q0002_54', 'ptend_q0002_55', 'ptend_q0002_56', 'ptend_q0002_57', 'ptend_q0002_58','ptend_q0002_59',
                  'ptend_q0003_0', 'ptend_q0003_1', 'ptend_q0003_2', 'ptend_q0003_3', 'ptend_q0003_4', 'ptend_q0003_5', 'ptend_q0003_6', 'ptend_q0003_7', 'ptend_q0003_8','ptend_q0003_9',
                   'ptend_q0003_10', 'ptend_q0003_11', 'ptend_q0003_12', 'ptend_q0003_13', 'ptend_q0003_14', 'ptend_q0003_15', 'ptend_q0003_16', 'ptend_q0003_17', 'ptend_q0003_18', 'ptend_q0003_19',
                   'ptend_q0003_20','ptend_q0003_21', 'ptend_q0003_22', 'ptend_q0003_23', 'ptend_q0003_24', 'ptend_q0003_25', 'ptend_q0003_26', 'ptend_q0003_27', 'ptend_q0003_28', 'ptend_q0003_29',
                   'ptend_q0003_30', 'ptend_q0003_31', 'ptend_q0003_32', 'ptend_q0003_33', 'ptend_q0003_34', 'ptend_q0003_35', 'ptend_q0003_36', 'ptend_q0003_37', 'ptend_q0003_38', 'ptend_q0003_39',
                   'ptend_q0003_40', 'ptend_q0003_41', 'ptend_q0003_42', 'ptend_q0003_43', 'ptend_q0003_44', 'ptend_q0003_45', 'ptend_q0003_46', 'ptend_q0003_47', 'ptend_q0003_48', 'ptend_q0003_49',
                   'ptend_q0003_50', 'ptend_q0003_51', 'ptend_q0003_52', 'ptend_q0003_53', 'ptend_q0003_54', 'ptend_q0003_55', 'ptend_q0003_56', 'ptend_q0003_57', 'ptend_q0003_58','ptend_q0003_59',
                  'ptend_u_0', 'ptend_u_1', 'ptend_u_2', 'ptend_u_3', 'ptend_u_4', 'ptend_u_5', 'ptend_u_6', 'ptend_u_7', 'ptend_u_8','ptend_u_9',
                   'ptend_u_10', 'ptend_u_11', 'ptend_u_12', 'ptend_u_13', 'ptend_u_14', 'ptend_u_15', 'ptend_u_16', 'ptend_u_17', 'ptend_u_18', 'ptend_u_19',
                   'ptend_u_20','ptend_u_21', 'ptend_u_22', 'ptend_u_23', 'ptend_u_24', 'ptend_u_25', 'ptend_u_26', 'ptend_u_27', 'ptend_u_28', 'ptend_u_29',
                   'ptend_u_30', 'ptend_u_31', 'ptend_u_32', 'ptend_u_33', 'ptend_u_34', 'ptend_u_35', 'ptend_u_36', 'ptend_u_37', 'ptend_u_38', 'ptend_u_39',
                   'ptend_u_40', 'ptend_u_41', 'ptend_u_42', 'ptend_u_43', 'ptend_u_44', 'ptend_u_45', 'ptend_u_46', 'ptend_u_47', 'ptend_u_48', 'ptend_u_49',
                   'ptend_u_50', 'ptend_u_51', 'ptend_u_52', 'ptend_u_53', 'ptend_u_54', 'ptend_u_55', 'ptend_u_56', 'ptend_u_57', 'ptend_u_58','ptend_u_59',
                  'ptend_v_0', 'ptend_v_1', 'ptend_v_2', 'ptend_v_3', 'ptend_v_4', 'ptend_v_5', 'ptend_v_6', 'ptend_v_7', 'ptend_v_8','ptend_v_9',
                   'ptend_v_10', 'ptend_v_11', 'ptend_v_12', 'ptend_v_13', 'ptend_v_14', 'ptend_v_15', 'ptend_v_16', 'ptend_v_17', 'ptend_v_18', 'ptend_v_19',
                   'ptend_v_20','ptend_v_21', 'ptend_v_22', 'ptend_v_23', 'ptend_v_24', 'ptend_v_25', 'ptend_v_26', 'ptend_v_27', 'ptend_v_28', 'ptend_v_29',
                   'ptend_v_30', 'ptend_v_31', 'ptend_v_32', 'ptend_v_33', 'ptend_v_34', 'ptend_v_35', 'ptend_v_36', 'ptend_v_37', 'ptend_v_38', 'ptend_v_39',
                   'ptend_v_40', 'ptend_v_41', 'ptend_v_42', 'ptend_v_43', 'ptend_v_44', 'ptend_v_45', 'ptend_v_46', 'ptend_v_47', 'ptend_v_48', 'ptend_v_49',
                   'ptend_v_50', 'ptend_v_51', 'ptend_v_52', 'ptend_v_53', 'ptend_v_54', 'ptend_v_55', 'ptend_v_56', 'ptend_v_57', 'ptend_v_58','ptend_v_59',
                  'cam_out_NETSW', 'cam_out_FLWDS', 'cam_out_PRECSC', 'cam_out_PRECC', 'cam_out_SOLS', 'cam_out_SOLL', 'cam_out_SOLSD', 'cam_out_SOLLD']

X_train = train_df[feature_columns][:70000]
y_train = train_df[target_columns][:70000]
X_test = train_df[feature_columns][70001:]
y_test = train_df[target_columns][70001:]

# Normalize the data
scaler_X = StandardScaler()
scaler_y = StandardScaler()

X_train_scaled = scaler_X.fit_transform(X_train)
y_train_scaled = scaler_y.fit_transform(y_train)
X_test_scaled = scaler_X.transform(X_test)
y_test_scaled = scaler_y.transform(y_test)

# Define the model
def build_model(input_shape, output_shape):
    inputs = layers.Input(shape=(input_shape,))
    x = layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(1e-4))(inputs)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.3)(x)
    x = layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(1e-4))(x)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.3)(x)
    outputs = layers.Dense(output_shape, activation='linear')(x)

    model = models.Model(inputs=inputs, outputs=outputs)
    model.compile(optimizer='adam', loss='mse', metrics=['mse'])
    return model

# Build the model
model = build_model(X_train_scaled.shape[1], y_train_scaled.shape[1])

# Define callbacks
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Custom R2 score callback
class R2ScoreCallback(tf.keras.callbacks.Callback):
    def __init__(self, validation_data):
        super().__init__()
        self.validation_data = validation_data

    def on_epoch_end(self, epoch, logs=None):
        val_pred = self.model.predict(self.validation_data[0])
        val_true = self.validation_data[1]
        r2 = r2_score(val_true, val_pred)
        print(f" - val_r2: {r2:.4f}")
        logs['val_r2'] = r2

# Train the model
history = model.fit(
    X_train_scaled, y_train_scaled,
    validation_split=0.2,
    epochs=100,
    batch_size=2048,
    callbacks=[early_stopping, R2ScoreCallback((X_test_scaled, y_test_scaled))],
    verbose=1
)

# Save the trained model
model.save('/path/to/best_model_dataframe.h5')

# Plotting learning curves
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['val_r2'], label='Validation R2 Score')
plt.title('Model R2 Score')
plt.xlabel('Epoch')
plt.ylabel('R2 Score')
plt.legend()

plt.tight_layout()
plt.show()

# Evaluate the model on the test set
test_loss, test_mse = model.evaluate(X_test_scaled, y_test_scaled)
print(f"Test Loss: {test_loss:.4f}")
print(f"Test MSE: {test_mse:.4f}")

# Make predictions on the test set
y_pred_scaled = model.predict(X_test_scaled)
y_pred = scaler_y.inverse_transform(y_pred_scaled)

# Evaluate the model on the test set
test_loss, test_mse = model.evaluate(X_test_scaled, y_test_scaled)
print(f"Test Loss: {test_loss:.4f}")
print(f"Test MSE: {test_mse:.4f}")

# Make predictions on the test set
y_pred_scaled = model.predict(X_test_scaled)
y_pred = scaler_y.inverse_transform(y_pred_scaled)

# Calculate R2 score for the test set
test_r2 = r2_score(y_test, y_pred)
print(f"Test R2 Score: {test_r2:.4f}")

"""#### XGBoost"""

import torch
from torch.utils.data import DataLoader, TensorDataset
from statistics import mean
from xgboost import XGBRegressor
from sklearn.metrics import r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error

train_df = p1.scan_csv(f'/kaggle/input/leap-atmospheric-physics-ai-climsim/train.csv')
fetch_size =1000
input_columns = train_df.columns[1:557]
output_columns = train_df.columns[557:]
X = train_df.select(p1.col(input_columns)).fetch(fetch_size)
Y = train_df.select(p1.col(output_columns)).fetch(fetch_size)
big = train_df.select(p1.col("*")).fetch(fetch_size)

def adicionar_agua_v_erro_info(big: p1.DataFrame, margen = 0.00005) ->p1.DataFrame:
    gravidade = 9.80665
    ptend_vapor = big.select(p1.col("^ptend_q0001.*$")).to_numpy()
    ptend_liquido = big.select(p1.col("^ptend_q0002.*$")).to_numpy()
    ptend_gelo = big.select(p1.col("^ptend_q0003.*$")).to_numpy()

    state_vapor = big.select(p1.col("^state_q0001.*$")).to_numpy()
    state_liquido = big.select(p1.col("^state_q0002.*$")).to_numpy()
    state_gelo = big.select(p1.col("^state_q0003.*$")).to_numpy()

    precipitacao_agua = big.select(p1.col("^cam_out_PRECC.*$")).to_numpy().flatten()
    precipitacao_neve = big.select(p1.col("^cam_out_PRECSC.*$")).to_numpy().flatten()


    pressao = big.select(p1.col("^state_ps.*$")).to_numpy().flatten()
    volume_inicial = np.sum(state_vapor + state_liquido + state_gelo,axis=1)*(pressao/gravidade)
    volume_predito = np.sum(ptend_vapor + ptend_liquido + ptend_gelo,axis=1)*(pressao/gravidade) + precipitacao_agua + precipitacao_neve


    erro_volumetrico = abs((volume_predito - volume_inicial)/volume_inicial +1)
    flag_erro_volumetrico = (erro_volumetrico >margen).astype(np.int8)
    big = big.with_columns([
        p1.Series("erro_volumetrico",erro_volumetrico),
        p1.Series("flag_erro_volumetrico",flag_erro_volumetrico)
    ])
    return big

def custom_log_transform(column):
    """Applies a log1p transformation to a given column with an adjustment for negative values."""
    min_value = column.min()
    transformed_column = np.log1p(column - min_value + 1)  # log1p is log(1 + x) which is stable for small x
    return transformed_column

def log_transformation(data):
    """Applies custom log transformation to each column in the data."""
    data_log_transformed = np.array(data, dtype="float64")
    for i in range(data.shape[1]):
        data_log_transformed[:, i] = custom_log_transform(data[:, i])
    return data_log_transformed

def preprocess_data(X, Y, transform=True, log=False):
    """Preprocesses the data by scaling and optionally applying log transformation."""
    X_np = X.to_numpy()
    Y_np = Y.to_numpy()

    if transform:
        # Standardize features
        X_scaler = StandardScaler().fit(X_np)
        Y_scaler = StandardScaler().fit(Y_np)

        X_transformed = X_scaler.transform(X_np)
        Y_transformed = Y_scaler.transform(Y_np)

        if log:
            X_transformed = log_transformation(X_transformed)
            Y_transformed = log_transformation(Y_transformed)

        # Clean up
        del X, Y, X_np, Y_np

    else:
        # Split data into training and validation sets without transformation
        #X_train, X_val, Y_train, Y_val = train_test_split(X_np, Y_np, test_size=0.2, random_state=42)

        # Clean up
        del X, Y, X_np, Y_np

    return X_transformed, Y_transformed

# Example usage
# Assuming X and Y are defined and are Polars DataFrames
X_process, Y_process = preprocess_data(X, Y, transform=True, log=False)
X_train, X_val, Y_train, Y_val = train_test_split(X_process, Y_process, test_size=0.2, random_state=42)

big=adicionar_agua_v_erro_info(big)

plt.boxplot(big["erro_volumetrico"])

sum(big["flag_erro_volumetrico"]>0)

plt.boxplot(big["erro_volumetrico"])

# Initialize model
model = XGBRegressor(
    n_estimators= 1150,
    max_depth=56,
    learning_rate=0.003,
    colsample_bytree=0.9,
    subsample=0.8,
    min_child_weight=1,
    random_state=42,
    objective='reg:squarederror',
    n_jobs=-1,  # Use all available cores
    tree_method='hist',
    device ='cuda',
)

r2_vector = []
mse_vector = []

y_pred_df = pd.DataFrame()
out_i =360
out_f=361
for i in range(out_i, out_f):

    model.fit(X_train, Y_train[:, i])
    y_pred = model.predict(X_val)
    y_pred_df[f'y_pred_{i}'] = y_pred

    # Calculate r
    r2 = r2_score(Y_val[:, i], y_pred)
    mse = mean_squared_error(Y_val[:, i], y_pred)
    print(f"{r2},{mse}")
    # Append scores to the lists
    r2_vector.append(r2)
    mse_vector.append(mse)

# Optionally convert lists to DataFrame for r2 and mse scores
r2_df = pd.DataFrame({'r2_score': r2_vector})
mse_df = pd.DataFrame({'mse_score': mse_vector})

# Now y_pred_df contains all the predictions for each target variable

columns = output_columns[out_i:out_f]
plt.figure(figsize=(20, 6))
plt.plot(columns, mse_vector, label='MSE', color='blue', marker='o')
plt.plot(columns, r2_vector, label='R²', color='red', marker='x')

plt.xlabel('Column Number')
plt.ylabel('Score')
plt.title('MSE and R² for each column')
plt.legend()
plt.grid(True)
plt.show()

"""#### Multi-Layer Perceptron"""

leap_raw_df = p1.scan_csv(f'/kaggle/input/leap-atmospheric-physics-ai-climsim/train.csv')
fetch_size =100000
input_columns = leap_raw_df.columns[1:557]
output_columns = leap_raw_df.columns[557:]

X = leap_raw_df.select(p1.col(input_columns)).fetch(fetch_size)
Y = leap_raw_df.select(p1.col(output_columns)).fetch(fetch_size)

from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import FunctionTransformer
constant_epson = 1
constant_epson_r = 1
def log_transform(x):
    return (np.log1p(x+constant_epson) +constant_epson_r)

# Convert Polars DataFrame to NumPy array
X_np = X.to_numpy()
Y_np = Y.to_numpy()

transformer_X = StandardScaler().fit(X_np)
transformer_Y = StandardScaler().fit(Y_np)

X_transformed = transformer_X.transform(X_np)
Y_transformed = transformer_Y.transform(Y_np)



# Log
#transformer =FunctionTransformer(log_transform)
#X_transformado=transformer.transform(X_np)
#Y_transformado=transformer.transform(Y_np)

print(X_transformed.dtype)
print(Y_transformed.dtype)

import torch
import torch.nn as nn
import torch.optim as optim

X_tensor = torch.tensor(X_transformed, dtype=torch.float64)
Y_tensor = torch.tensor(Y_transformed, dtype=torch.float64)

from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

X_train, X_val, Y_train,Y_val = train_test_split(X_tensor, Y_tensor, test_size=0.2, random_state=42)
train_dataset = TensorDataset(X_train, Y_train)
val_dataset = TensorDataset(X_val, Y_val)

def create_dataloader(dataset, batch_size):
    return DataLoader(dataset, batch_size=batch_size, shuffle=True)

import optuna
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
import polars as pl

def objective(trial):
    # Define the actual input size and output size based on your data
    input_size = 556  # Number of features in X
    output_size = 368  # Number of targets in Y

    # Tune the number of hidden layers and units per layer
    hidden_layers = trial.suggest_int('hidden_layers', 3, 8)
    hidden_units = trial.suggest_int('hidden_units', 512, 1024)
    batch_size = trial.suggest_int('batch_size', 1536, 4608)
    learning_rate = trial.suggest_loguniform('learning_rate', 2.5e-4, 2.5e-2)

    X_train_tensor = torch.tensor(X_train, dtype=torch.float64)
    Y_train_tensor = torch.tensor(Y_train, dtype=torch.float64)
    X_val_tensor = torch.tensor(X_val, dtype=torch.float64)
    Y_val_tensor = torch.tensor(Y_val, dtype=torch.float64)

    # Define the architecture of the MLP
    class MLPRegressor(nn.Module):
        def __init__(self, input_size, output_size, hidden_layers, hidden_units):
            super(MLPRegressor, self).__init__()
            layers = []
            layers.append(nn.Linear(input_size, hidden_units, dtype=torch.float64))
            layers.append(nn.LeakyReLU(0.15))
            for _ in range(hidden_layers - 1):
                layers.append(nn.Linear(hidden_units, hidden_units, dtype=torch.float64))
                layers.append(nn.LeakyReLU(0.15))
            layers.append(nn.Linear(hidden_units, output_size, dtype=torch.float64))
            self.model = nn.Sequential(*layers)

        def forward(self, x):
            return self.model(x)

    model = MLPRegressor(input_size, output_size, hidden_layers, hidden_units).double()
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    def create_dataloader(X, Y, batch_size):
        dataset = torch.utils.data.TensorDataset(X, Y)
        return torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

    train_loader = create_dataloader(X_train_tensor, Y_train_tensor, batch_size)
    val_loader = create_dataloader(X_val_tensor, Y_val_tensor, batch_size)

    # Train the model
    def train(model, criterion, optimizer, train_loader, val_loader, epochs=4):
        model.train()
        for epoch in range(epochs):
            for inputs, targets in train_loader:
                optimizer.zero_grad()
                outputs = model(inputs)
                loss = criterion(outputs, targets)
                loss.backward()
                optimizer.step()

        # Evaluate on the validation set
        model.eval()
        val_preds = []
        val_targets = []
        with torch.no_grad():
            for val_inputs, val_targets_batch in val_loader:
                val_outputs = model(val_inputs)
                val_preds.append(val_outputs)
                val_targets.append(val_targets_batch)

        val_preds = torch.cat(val_preds).cpu().numpy()
        val_targets = torch.cat(val_targets).cpu().numpy()
        val_preds[:, 61] = 0
        mse = mean_squared_error(val_targets, val_preds)

        r2_values = []
        for i in range(output_size):
            r2 = r2_score(val_targets[:, i], val_preds[:, i])
            r2_values.append(r2)
        print(f"R² for column 62: {r2_values[61]}")

        # Calculate the average R² excluding the value at index 61
        r2_values_excluding_61 = [r2 for idx, r2 in enumerate(r2_values) if idx != 61]
        avg_r2_excluding_61 = sum(r2_values_excluding_61) / len(r2_values_excluding_61)
        print("Average R² excluding column 61:", avg_r2_excluding_61)
        return avg_r2_excluding_61

    avg_r2_excluding_61 = train(model, criterion, optimizer, train_loader, val_loader)
    return avg_r2_excluding_61

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=100)

print("Best hyperparameters: ", study.best_params)
print("Best value (MSE): ", study.best_value)

import optuna
import warnings
warnings.filterwarnings("ignore")

import optuna.visualization as vis
import matplotlib.pyplot as plt

importance_plot = vis.plot_param_importances(study)
importance_plot.show()

opt_history_plot = vis.plot_optimization_history(study)
opt_history_plot.show()

parallel_plot = vis.plot_parallel_coordinate(study)
parallel_plot.show()

contour_plot = vis.plot_contour(study)
contour_plot.show()

conda install git

conda install boto3

import boto3
import os
import os

os.environ['AWS_ACCESS_KEY_ID'] = ''
os.environ['AWS_SECRET_ACCESS_KEY'] = ''
os.environ['AWS_DEFAULT_REGION'] = 'us-east-2'

def upload_to_s3(file_path, bucket_name, s3_key):
    # Create an S3 client
    s3 = boto3.client('s3')

    try:
        # Upload the file
        s3.upload_file(file_path, bucket_name, s3_key)
        print(f"Successfully uploaded {file_path} to {bucket_name}/{s3_key}")
    except Exception as e:
        print(f"Error uploading file: {str(e)}")

# Example usage
local_file_path = '../input/leap-atmospheric-physics-ai-climsim/train.csv'
s3_bucket_name = 'capstone-project-ucsd-mle-bootcamp'
s3_file_key = 'data.csv'

upload_to_s3(local_file_path, s3_bucket_name, s3_file_key)

